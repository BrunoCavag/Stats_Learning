{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Need to Know Just one Test\n",
    "\n",
    "![test](https://miro.medium.com/max/630/1*NqycnGXFLuvypvOQ-vfw4Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am here to reassure you: as a data professional, there is only one test that you need to know. Not because 1 test is important and the other 103 are negligible. But because:**\n",
    "\n",
    "## All the statistical tests are in reality the same one test!\n",
    "\n",
    "We will solve 4 very diverse statistical problems. And we will solve them always using the same exact algorithm.\n",
    "\n",
    "1. You have thrown a die 10 times. You got [1, 1, 1, 1, 1, 2, 2, 2, 3, 3]. Is the die loaded?\n",
    "2. Your friend claims that some Scrabble tiles fell out of the bag and, coincidentally, the letters formed a real word: “F-E-A-R”. You suspect that your friend is just trying make fun of you. Is your friend lying?\n",
    "3. In a customer satisfaction survey, 100 customers gave an average rating of 3.00 to product A and 2.63 to product B. Is this difference significant?\n",
    "4. You trained a binary classification model. It has an area under the ROC curve of 70% on your test set (made of 100 observations). Is the model significantly better than random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a hypothesis — called “null hypothesis” — and you want to put it to the test. Thus, you ask yourself:\n",
    "*“If the hypothesis was true, how often would I get an outcome as suspect as the outcome that I actually had?”*\n",
    "\n",
    "In the example of the die, the question becomes: “If the die was fair, how often would I get a sequence as unexpected as [2, 2, 2, 2, 2, 4]?” Since you are asking “how often”, the answer must necessarily be a number between 0 and 1, where 0 means never and 1 means always.\n",
    "\n",
    "**In statistics, this “how often” is called “p-value”.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ingredients of statistical testing\n",
    "\n",
    "Reading the previous paragraph, you may have guessed that we need two ingredients:\n",
    "\n",
    "- The distribution of the possible outcomes, depending on the null hypothesis.\n",
    "- A measure of the “unexpectedness” of any outcome.\n",
    "\n",
    "## A unique statistical test\n",
    "But how do we do it in Python? The algorithm is the following:\n",
    "\n",
    "1. Define a function draw_random_outcome. This function should return the outcome of a random trial, given that the null hypothesis is true. It may be a single number, an array, a list of arrays, an image, practically anything: it depends on the specific case.\n",
    "\n",
    "2. Define a function unexp_score (which stands for “unexpectedness score”). The function should take an experiment outcome as input, and return a single number. This number must be a score of how unexpected the outcome is, assuming it was generated under the null hypothesis. The score may be positive, negative, integer, or float, it doesn’t matter. The only property it must have is the following: the unlikelier the outcome is, the higher this score must be.\n",
    "\n",
    "3. Run many times (e.g. 10,000 times) the function draw_random_outcome (defined at point 1) and, for each random outcome, compute its unexp_score (defined at point 2). Store all the scores in an array called random_unexp_scores.\n",
    "\n",
    "4. Compute unexp_score of the observed outcome, and call it observed_unexp_score.\n",
    "\n",
    "5. Compute how many random outcomes are more unexpected than the observed outcome. That is to say, count how many elements of random_unexp_scores are higher than observed_unexp_score. This is the p-value.\n",
    "\n",
    "\n",
    "**The first two steps are the only ones that require a bit of creativity**, depending on the specific case, while steps 3, 4, and 5 are purely mechanical.\n",
    "\n",
    "Now, to make it more concrete, let’s go through the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Rolling A Die\n",
    "\n",
    "We have launched a die 10 times and obtained this outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "observed_outcomes = np.array([1,1,1,1,1,2,2,2,3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the die is fair. Under this hypothesis, it’s easy to extract random outcomes: it’s enough to use Numpy’s random.choice. So, this is the first step of our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "def draw_random_outcome():\n",
    "  return np.random.choice([1,2,3,4,5,6], size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to define a function called unexp_score that should assign a score of unexpectedness to each possible outcome.\n",
    "\n",
    "If the die is fair, we expect each face to appear on average one sixth of the time. So we should check the distance between the observed frequency of each face and 1/6. Then, to obtain a single score, we should take the average. In this way, the higher the average distance from one sixth, the more unexpected is the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "def unexp_score(outcome):\n",
    "  outcome_distribution = np.array([np.mean(outcome == face) for face in [1,2,3,4,5,6]])\n",
    "  return np.mean(np.abs(outcome_distribution - 1/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "n_iter = 10000\n",
    "random_unexp_scores = np.empty(n_iter)\n",
    "for i in range(n_iter):\n",
    "  random_unexp_scores[i] = unexp_score(draw_random_outcome())\n",
    "# step 4\n",
    "observed_unexp_score = unexp_score(observed_outcomes)\n",
    "# step 5\n",
    "pvalue = np.sum(random_unexp_scores >= observed_unexp_score) / n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0229"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unexp_scores](https://miro.medium.com/max/630/1*C7LRHNOVL1Z8OAwIim2eiA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Scrabble mystery\n",
    "\n",
    "\n",
    "Your friend claims that some Scrabble tiles fell out of the bag and, coincidentally, the letters formed a real word: “F-E-A-R”. You suspect that your friend is teasing you. How to check statistically if your friend is lying?\n",
    "\n",
    "First of all, the observed outcome is a sequence of letters, therefore a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_outcome = 'FEAR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the bag contained the 26 letters of the alphabet. The null hypothesis is that a random number of letters (between 1 and 26) fell out of the bag in random order. So, we will have to use Numpy’s random for both the number of letters and the choice of the letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "import string\n",
    "\n",
    "def draw_random_outcome():\n",
    "  size=np.random.randint(low=1, high=27)\n",
    "  return ''.join(np.random.choice(list(string.ascii_uppercase), size=size, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to evaluate unexpectedness in this scenario?\n",
    "\n",
    "In general, it’s reasonable to expect that the more letters fall from the bag, the less likely is to obtain a real word.\n",
    "\n",
    "Therefore, we can use this rule: if the string is an existing word, then its score will be the length of the word. If the string is not a real word, then its score will be minus the length of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from english_words import english_words_set\n",
    "\n",
    "english_words_set = [w.upper() for w in english_words_set]\n",
    "def unexp_score(outcome):\n",
    "  is_in_dictionary = outcome in english_words_set\n",
    "  return (1 if is_in_dictionary else -1) * len(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "n_iter = 10000\n",
    "random_unexp_scores = np.empty(n_iter)\n",
    "for i in range(n_iter):\n",
    "  random_unexp_scores[i] = unexp_score(draw_random_outcome())\n",
    "# step 4\n",
    "observed_unexp_score = unexp_score(observed_outcome)\n",
    "# step 5\n",
    "pvalue = np.sum(random_unexp_scores >= observed_unexp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3. Difference between two means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_a = np.repeat([1,2,3,4,5], 20)\n",
    "product_b = np.array([1]*27+[2]*25+[3]*19+[4]*16+[5]*13)\n",
    "observed_outcome = np.mean(product_a) - np.mean(product_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "def draw_random_outcome():\n",
    "  pr_a, pr_b = np.random.permutation(np.hstack([product_a, product_b])).reshape(2,-1)\n",
    "  return np.mean(pr_a) - np.mean(pr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "def unexp_score(outcome):\n",
    "  return np.abs(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "n_iter = 10000\n",
    "random_unexp_scores = np.empty(n_iter)\n",
    "for i in range(n_iter):\n",
    "  random_unexp_scores[i] = unexp_score(draw_random_outcome())\n",
    "# step 4\n",
    "observed_unexp_score = unexp_score(observed_outcome)\n",
    "# step 5\n",
    "pvalue = np.sum(random_unexp_scores >= observed_unexp_score)/ n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4. Area under the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.random.choice([0,1], size=100, p=[.9,.1])\n",
    "proba_test = np.random.uniform(low=0, high=1, size=100)\n",
    "observed_outcome = .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "def draw_random_outcome():\n",
    "  return roc_auc_score(y_test, np.random.permutation(proba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "def unexp_score(outcome):\n",
    "  return np.abs(outcome - .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "n_iter = 10000\n",
    "random_unexp_scores = np.empty(n_iter)\n",
    "for i in range(n_iter):\n",
    "  random_unexp_scores[i] = unexp_score(draw_random_outcome())\n",
    "# step 4\n",
    "observed_unexp_score = unexp_score(observed_outcome)\n",
    "# step 5\n",
    "pvalue = np.sum(random_unexp_scores >= observed_unexp_score) / n_iter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('bru_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7033ad42021d8fde750941f7374709dc7fad13ee27a9cd8f3ab36e378bdd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
